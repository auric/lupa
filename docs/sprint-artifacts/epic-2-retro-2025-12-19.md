# Epic 2 Retrospective: Rich UX & Agent Mode Integration

**Date:** December 19, 2025
**Facilitator:** Bob (SM Agent)
**Format:** Party Mode (Multi-agent team simulation + Subagent Research)
**Epic Status:** ‚úÖ DONE (3/3 stories completed)

---

## üìä Epic Summary

**Epic 2: Rich UX & Agent Mode Integration** delivered rich progress visualization, follow-up suggestions, and Agent Mode tool registration.

### Stories Delivered

| Story | Title                       | Status  | Key Deliverables                                                                                            |
| ----- | --------------------------- | ------- | ----------------------------------------------------------------------------------------------------------- |
| 2.1   | Rich Progress Visualization | ‚úÖ Done | `ChatResponseBuilder` migration, `stream.filetree()`, tool-specific progress, system prompt UX enhancements |
| 2.2   | Follow-up Suggestions       | ‚úÖ Done | `ChatFollowupProvider`, metadata population, contextual chips                                               |
| 2.3   | Register Agent Mode Tool    | ‚úÖ Done | `LanguageModelToolProvider`, `lupa_getSymbolsOverview` exposed to Agent Mode                                |

### Metrics

- **Test Coverage:** 894 tests passing (added 16 from Epic 1's 878)
- **Duration:** 2 days (December 17-19, 2025)
- **Story Velocity:** 1.5 stories/day

---

## üåü What Went Well

### Technical Excellence

**1. ChatResponseBuilder Migration (Story 2.1)**

> _"Finally consolidated all extension-generated messages through the builder. Consistent UX formatting across errors, empty states, and cancellation."_ ‚Äî Amelia (Dev)

- Migrated 5 inline formatting patterns to `ChatResponseBuilder`
- Added `addErrorSection()` method for error display
- All extension-generated messages now follow UX specification

**2. System Prompt UX Enhancements (Story 2.1)**

> _"The tone guidelines and certainty principle are exactly what we needed. LLM output is now more supportive and honest about uncertainty."_ ‚Äî Mary (UX)

- Added `<tone_guidelines>` for supportive, non-judgmental language
- Added `<certainty_principle>` for exception-based uncertainty flagging
- Made "What's Good" section mandatory

**3. Tool-Specific Progress Messages (Story 2.1)**

> _"Users now see 'üìÇ Reading src/auth/handler.ts...' instead of generic 'Tool call in progress'. Much better UX."_ ‚Äî Charlie (Dev)

- 12 tool-specific message templates
- Uses emoji from `chatEmoji.ts` consistently
- Removed noisy turn indicators ("Turn 1/100...")

**4. File Tree Display (Story 2.1)**

> _"The `stream.filetree()` integration shows changed files before analysis starts. Users immediately see what's being reviewed."_ ‚Äî Dana (QA)

- `buildFileTree()` utility transforms diff paths to hierarchical structure
- Collapsible file tree in chat response

**5. Agent Mode Tool Registration (Story 2.3)**

> _"Clean architecture. The LanguageModelToolProvider wraps GetSymbolsOverviewTool exactly as architecture.md specified."_ ‚Äî Winston (Architect)

- `lupa_getSymbolsOverview` exposed to Copilot Agent Mode
- Graceful degradation if Language Model API unavailable
- Full schema alignment with existing tool

### Process Excellence

**6. Party Mode Research Collaboration**

- Used subagents for parallel Epic 3 validation
- Identified 6 significant issues before implementation starts
- Research-driven story preparation

---

## üî¥ CRITICAL ISSUES DISCOVERED

### Issue 1: Follow-up Chips Lead to Dead End

**Severity:** üî¥ CRITICAL
**Discovered by:** Subagent research + Igor

**Problem:** When users click follow-up chips (e.g., "üîß Fix Guidance"), the prompt is sent to `@lupa` **without a slash command**. The current handler only routes `/branch` and `/changes`, falling through to "Commands coming soon!" for everything else.

**Evidence (chatParticipantService.ts):**

```typescript
if (request.command === 'branch') { return this.handleBranchCommand(...); }
if (request.command === 'changes') { return this.handleChangesCommand(...); }
// Falls through to:
stream.markdown('Lupa chat participant registered. Commands coming soon!');
```

**Impact:** Follow-up chips are **completely broken**. Users see follow-up suggestions but clicking them produces a useless placeholder message.

**Fix Target:** Story 3.1 (Exploration Mode) - Implement `handleExplorationMode()` for no-command requests.

---

### Issue 2: Emoji Detection Is Fundamentally Flawed

**Severity:** üî¥ CRITICAL
**Discovered by:** Igor + Code analysis

**Problem:** `analyzeResultContent()` detects security (üîí) and testing (üß™) issues by emoji presence, but:

1. These emoji are **NOT mentioned in the system prompt** - LLM doesn't know to use them
2. LLM may write "Security vulnerability:" without any emoji ‚Üí no follow-up chip shown
3. Detection depends on uncontrolled free-form text generation

**Evidence:**

- `üîí` defined in `chatEmoji.ts` line 29 as `SECTION.security`
- System prompt (`toolAwareSystemPromptGenerator.ts`) does NOT instruct LLM to use `üîí` or `üß™`
- False negatives are guaranteed when LLM doesn't happen to use our exact emoji

**Impact:** Security and testing follow-up chips will rarely appear because detection is unreliable.

**Fix Options:**

1. **Option A (Quick Fix):** Add `üîí` and `üß™` emoji requirements to system prompt
2. **Option B (Better):** Use keyword detection as fallback ("security", "test")
3. **Option C (Best - Recommended):** LLM-generated follow-ups via tool call

**Recommendation:** Option C - Have LLM call `generate_followups` tool with contextual suggestions.

---

## üü† HIGH PRIORITY ISSUES

### Issue 3: History Not Integrated - Follow-ups Lack Context

**Severity:** üü† HIGH
**Source:** Epic 1 Retrospective + Igor

**Problem:** Conversation history is intentionally ignored for commands (correct), but also ignored for follow-ups (incorrect). When a follow-up runs, it has no context about the previous analysis.

**Design Decision (from Epic 1 Retro):**
| Mode | Include History? |
|------|------------------|
| `/branch`, `/changes` | ‚ùå NO (fresh analysis) |
| `@lupa` (exploration) | ‚úÖ YES |
| Follow-up chips | ‚úÖ YES |

**Fix Target:** Story 3.2 (Conversation History Integration)

---

### Issue 4: ChatContextManager Does Not Exist

**Severity:** üü† HIGH
**Discovered by:** Subagent research

**Problem:** `ChatContextManager` is specified in architecture.md but doesn't exist. It's a required deliverable for Story 3.2.

**Required Functionality:**

- Token budget tracking via `model.maxInputTokens`
- Sliding window truncation (newest-first priority)
- Reserve 4000 tokens for output

**Fix Target:** Story 3.2 deliverable

---

## üü° MEDIUM PRIORITY ISSUES

### Issue 5: getDefaultFollowups Provides No Value

**Severity:** üü° MEDIUM
**Source:** Igor

**Problem:** Default follow-ups when metadata is unavailable are meaningless:

```typescript
{ prompt: "Ask a follow-up question about these changes", label: "‚ùì Ask Question" },
{ prompt: "What should I focus on next?", label: "üéØ Next Steps" },
```

These are meta-instructions, not actionable prompts. They also lead to the "Commands coming soon!" dead end (Issue 1).

**Recommendation:** Either remove default follow-ups or make them trigger specific exploration queries.

---

### Issue 6: Disambiguation Not Configured

**Severity:** üü° MEDIUM
**Discovered by:** Subagent research

**Problem:** package.json has no `disambiguation` property for the chat participant. Users must explicitly type `@lupa` - no auto-routing.

**Fix Target:** Story 3.3 (simple JSON change)

---

## üìã Action Items

| ID  | Action                                                              | Owner | Target                | Priority |
| --- | ------------------------------------------------------------------- | ----- | --------------------- | -------- |
| A1  | Implement exploration mode handler (fixes Issue 1, 5)               | Dev   | Story 3.1             | üî¥ P0    |
| A2  | Implement LLM-generated follow-ups via tool call (fixes Issue 2)    | Dev   | Story 3.2 or separate | üî¥ P0    |
| A3  | Create ChatContextManager (fixes Issue 4)                           | Dev   | Story 3.2             | üü† P1    |
| A4  | Integrate conversation history for exploration mode (fixes Issue 3) | Dev   | Story 3.2             | üü† P1    |
| A5  | Add disambiguation configuration to package.json (fixes Issue 6)    | Dev   | Story 3.3             | üü° P2    |
| A6  | Update Story 2.2 epics.md to document emoji detection limitation    | SM    | Immediate             | üìù Doc   |

---

## üîç Epic 3 Readiness Assessment

### Subagent Research Summary

| Story                   | Readiness  | Blocker?                    | Estimated Effort |
| ----------------------- | ---------- | --------------------------- | ---------------- |
| 3.1 Exploration Mode    | ‚úÖ Ready   | No                          | 2-4 hours        |
| 3.2 History Integration | ‚ö†Ô∏è Partial | No (it creates the blocker) | 2-3 days         |
| 3.3 Disambiguation      | ‚úÖ Ready   | No                          | 1-2 hours        |

### Story 3.1: Exploration Mode

**Current State:**

- Placeholder handler exists ("Commands coming soon!")
- `ConversationRunner` already works without diff context
- Tools are available and wired

**Required Work:**

1. Create exploration system prompt (remove PR-specific language)
2. Implement `handleExplorationMode()` method
3. Route no-command requests to exploration handler

**No Blockers** - Can start immediately.

---

### Story 3.2: Conversation History Integration

**Current State:**

- `context.history` is unused (correct for commands)
- `ChatContextManager` does not exist
- Token counting infrastructure exists

**Required Work:**

1. Create `ChatContextManager` class
2. Implement history extraction from `ChatContext.history`
3. Token budget tracking with sliding window
4. Integrate history in exploration mode only

**Dependencies:** Story 3.1 should be completed first (exploration mode is where history is used).

---

### Story 3.3: Disambiguation Auto-routing

**Current State:**

- No disambiguation configuration in package.json
- `isParticipantDetected` requires proposed API

**Required Work:**

1. **Phase A (immediate):** Add disambiguation JSON to package.json
2. **Phase B (deferred):** Add `isParticipantDetected` handling when API stabilizes

**No Blockers** - Phase A is a JSON-only change.

---

## üöÄ Next Epic Preparation

### Epic 3: Exploration Mode & Polish

**Status:** Ready to start
**Dependencies:** Epic 2 ‚úÖ complete (with known issues)

| Story | Title                            | Dependencies | Fixes Issues | Status           |
| ----- | -------------------------------- | ------------ | ------------ | ---------------- |
| 3.1   | Exploration Mode                 | Epic 2       | Issues 1, 5  | Ready            |
| 3.2   | Conversation History Integration | Story 3.1    | Issues 3, 4  | Ready after 3.1  |
| 3.3   | Disambiguation Auto-routing      | Story 1.1    | Issue 6      | Ready (parallel) |

**Critical Path:**

1. **Story 3.1 FIRST** - Fixes the broken follow-up UX
2. **Story 3.2 SECOND** - Enables history-aware follow-ups
3. **Story 3.3 PARALLEL** - Can be done alongside 3.1 or 3.2

---

## üéØ Recommendations for Epic 3

### 1. Redesign Follow-up Generation (High Priority)

**Current:** Emoji detection in `analyzeResultContent()` - fragile and unreliable

**Proposed:** LLM-generated follow-ups via tool call

```typescript
// New tool for LLM to call at end of analysis
{
    name: "generate_followups",
    schema: z.object({
        followups: z.array(z.object({
            label: z.string(),  // "Deep dive into auth"
            prompt: z.string(), // "Explain the auth vulnerability in login.ts:42"
            type: z.enum(['security', 'testing', 'fix', 'explain'])
        })).max(4)
    })
}
```

**Benefits:**

- Follow-ups are contextually relevant to actual findings
- LLM can reference specific files/issues
- No fragile emoji detection
- Scales to future enhancements

### 2. Update Story 3.2 Scope

Add explicit ACs for:

- LLM-generated follow-ups (or separate story)
- Deprecation of emoji-based detection
- Default follow-ups improvement

### 3. Consider Splitting Story 3.2

Current scope may be too large:

- **3.2a:** Exploration mode history integration
- **3.2b:** Token budget management (ChatContextManager)
- **3.2c:** LLM-generated follow-ups

---

## üë• Retrospective Participants (Party Mode)

| Agent   | Role          | Key Contribution                        |
| ------- | ------------- | --------------------------------------- |
| Bob     | Scrum Master  | Facilitation, subagent coordination     |
| Winston | Architect     | Design decision validation              |
| Charlie | Senior Dev    | Technical analysis, code review         |
| Amelia  | Dev           | Implementation insights                 |
| Dana    | QA            | Test coverage, edge case identification |
| Mary    | UX Designer   | UX impact assessment                    |
| Alice   | Product Owner | Scope alignment, prioritization         |
| Igor    | Project Lead  | Pre-identified 6 key issues             |

---

## üìà Epic Velocity

| Metric                | Value           |
| --------------------- | --------------- |
| Stories Planned       | 3               |
| Stories Completed     | 3               |
| Duration              | 2 days          |
| Velocity              | 1.5 stories/day |
| Tests Added           | 16              |
| Total Tests           | 894             |
| Critical Issues Found | 2               |
| High Issues Found     | 2               |
| Medium Issues Found   | 2               |

---

## üìö Documents Updated

| Document                                   | Changes                                                                 |
| ------------------------------------------ | ----------------------------------------------------------------------- |
| `docs/sprint-artifacts/sprint-status.yaml` | To be updated: epic-2 ‚Üí done, epic-2-retrospective ‚Üí completed          |
| `docs/epics.md`                            | Consider updating Story 2.2 with emoji detection limitation note        |
| Architecture consideration                 | LLM-generated follow-ups tool should be added to architecture decisions |

---

## Key Learnings

### 1. Validate Integration End-to-End

The follow-up chips were implemented and tested in isolation, but the integration path (click chip ‚Üí send prompt ‚Üí handle request) was not tested. The dead-end was only discovered during retrospective research.

**Lesson:** Always test the full user journey, not just individual components.

### 2. Don't Rely on Uncontrolled LLM Output

Emoji detection depends on LLM following a format that isn't enforced. This is fundamentally unreliable.

**Lesson:** For critical features, use structured output (tool calls, JSON mode) rather than free-form text parsing.

### 3. Research Before Shipping

The subagent research approach uncovered 6 issues in ~10 minutes. This should happen BEFORE shipping, not after.

**Lesson:** Budget time for integration validation research before marking epics as done.

---

_Retrospective facilitated in Party Mode by SM agent (Bob) with Subagent Research using BMAD workflow v6.0.0-alpha.16_
